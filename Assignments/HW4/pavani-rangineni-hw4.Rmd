---
title: "CS 422 HW4"
author: "Pavani Rangineni"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    toc_float: yes
---

### Part 2-1-A

```{r}
library("dplyr")
set.seed(1122)

adult_train<- read.csv('adult-train.csv', 
                           header = TRUE)
adult_test<- read.csv('adult-test.csv', header = TRUE)


filterTrainData <- which(adult_train$occupation == "?" | adult_train$native_country == "?" | adult_train$workclass == "?")
adult_train.df <- adult_train %>% filter(row_number() %in% filterTrainData)


filterTestData <- which(adult_test$occupation == "?" | adult_test$native_country == "?" | adult_test$workclass == "?")
adult_test.df <- adult_test %>% filter(!row_number() %in% filterTestData)


adult_train.df
adult_test.df
```

### Part 2-1-B

```{r}
library(rpart)
library(rpart.plot)
model <- rpart(income ~ ., data=adult_train, method="class")
print(model)
rpart.plot(model, extra=104, fallen.leaves=T, type=4, main="Income Train Dataset Decision Tree")
```

### Part 2-1-B-i
```{r}
print(model$variable.importance[1:3])
paste("The  top three important predictors in the model are: relationship, marital_status, capital_gain")
```

### Part 2-1-B-ii

```{r}
# The first split is done on relationship. The predicted class of the first node is "<=50K".
# The distribution of observations between the “<=50K” and “>50K” classes at first node are 22651 and 7510
```

### Part 2-1-C
```{r}
library(caret)
predict.df <- predict(model, adult_test, type="class")
confusionmat_table <- table(predict.df, adult_test$income)
confusionmat <- confusionMatrix(predict.df, as.factor(adult_test$income))
confusionmat
```
### Part 2-1-C-i
```{r}
sensitivity.df <- sensitivity(confusionmat_table)
specificity.df <- specificity(confusionmat_table)
balanced.df <- ( sensitivity.df + specificity.df )/2
paste("The balanced accuracy is: ", round((balanced.df), digits = 3))
```
### Part 2-1-C-ii
```{r}
balanced_errrate.df <- (1 - balanced.df)
paste("The balanced error rate:", round((balanced_errrate.df), digits=3))
```
### Part 2-1-C-iii
```{r}
# Balanced accuracy is a metric we can use to assess the performance of a classification model. 
# It is calculated as Balanced accuracy = (Sensitivity + Specificity) / 2
# Sensitivity: The “true positive rate” – the percentage of positive cases the model is able to detect
# Specificity: The “true negative rate” – the percentage of negative cases the model is able to detect
# This metric is particularly useful when the two classes are imbalanced – that is, one class appears much more than the other
```

### Part 2-1-C-iv
```{r}
library(ROCR)
# AUC - AUC stands for "Area under the ROC Curve." That is, AUC measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1).
# ROC CURVE - ROC (Receiver Operator Characteristic Curve) can help in deciding the best threshold value. A ROC curve is plotted with FPR on the X-axis and TPR on the y-axis.
pred.rocr <- predict(model, newdata=adult_test, type="prob")[,2]
roc.pred <- prediction(pred.rocr, adult_test$income)
roc.perf <- performance(roc.pred, "tpr", "fpr")
auc <- performance(roc.pred, measure = "auc")
paste("AUC: ", round((auc@y.values[[1]]), digits = 3))
plot(roc.perf, colorize=T, lwd=3)
abline(0,1)
```
### Part 2-1-D
```{r}

base_acc <- mean(predict.df == adult_test$income)
printcp(model)
plotcp(model)
paste('The optimal CP value is', model$cptable[which.min(model$cptable[,"xerror"])])
model_prune <- prune(model, cp=0.01)
predict_prune <- predict(model_prune, adult_test, type="class")
prune_acc <- mean(predict_prune == adult_test$income)
prune_data <- data.frame(base_acc, prune_acc)
prune_data
# The accuracy of the model on the test data is equal when the tree is pruned, so there won't be any benefit of pruning on this model. 
```

### Part 2-1-E-i
```{r}
set.seed(1122)
train_table <- table(adult_train$income)
# By looking at the summary of the model the root node has 75.11% of training data as "<=50k" and 24.899% of training data as ">50K"
paste("The total number of observations are in the class <=50K: ", train_table[1])
paste("The total number of observations are in the class >50K: ", train_table[2])
```
### Part 2-1-E-ii
```{r}
set.seed(1122)
lincome_train <- which(adult_train$income == "<=50K")
gincome_train <- which(adult_train$income == ">50K")
sample <-  sample(lincome_train, size=length(gincome_train))
new_adtrain <- adult_train[c(sample, gincome_train),]
new_adtrain
table(new_adtrain$income)
```
### Part 2=1-E-iii
```{r}
library(rpart)
library(rpart.plot)
new_model <- rpart(income ~ ., data=new_adtrain, method="class")
new_predictor <- predict(new_model, adult_test, type="class")
new_confusion_mattable <- table(new_predictor, adult_test$income)
new_confusion_mat <- confusionMatrix(new_predictor, as.factor(adult_test$income))
new_confusion_mat
```
### Part 2-1-E-iii-i
```{r}
new_sensitivity <- sensitivity(new_confusion_mattable) 
new_specificity <- specificity(new_confusion_mattable)
new_balanced <- ( new_sensitivity + new_specificity )/2
paste("The balanced accuracy is: ", round((new_balanced), digits = 3))
```

### Part 2-1-E-iii-ii
```{r}
new_balanced_error_rate <- (1 - new_balanced)
paste("The balanced error rate:", round((new_balanced_error_rate), digits=3))
```
### Part 2-1-E-iii-iii
```{r}
# Balanced accuracy is a metric we can use to assess the performance of a classification model
# It is calculated as: Balanced accuracy = (Sensitivity + Specificity) / 2
# Sensitivity: The “true positive rate” – the percentage of positive cases the model is able to detect
# Specificity: The “true negative rate” – the percentage of negative cases the model is able to detect
# This metric is particularly useful when the two classes are imbalanced – that is, one class appears much more than the other
```

### Part 2-1-E-iii-iv
```{r}
# AUC - AUC stands for "Area under the ROC Curve." That is, AUC measures the entire two-dimensional area underneath the entire ROC curve from (0,0) to (1,1).
# ROC CURVE - ROC (Receiver Operator Characteristic Curve) can help in deciding the best threshold value. A ROC curve is plotted with FPR on the X-axis and TPR on the y-axis.

new_pred.rocr <- predict(new_model, newdata=adult_test.df, type="prob")[,2]
new_roc.pred <- prediction(new_pred.rocr, adult_test.df$income)
new_roc.perf <- performance(new_roc.pred, "tpr", "fpr")
new_auc <- performance(new_roc.pred, measure = "auc")
paste("AUC: ", round((new_auc@y.values[[1]]), digits = 3))
plot(new_roc.perf, colorize=T, lwd=3)
abline(0,1)

```

### Part 2-1-F
```{r}
# The balanced accuracy, sensitivity, specificity, positive predictive value and AUC of the model used in 2.1 (c)
# Sensitivity                  : 0.949        
# Specificity                  : 0.505         
# Positive Predictive Value    : 0.861
# Balanced Accuracy            : 0.727
# AUC                          : 0.843
# Balanced Error Rate          : 0.275
# The balanced accuracy, sensitivity, specificity, positive predictive value and AUC of the model used in 2.1 (e)
# Sensitivity                  : 0.782         
# Specificity                  : 0.828         
# Positive Predictive Value    : 0.936
# Balanced Accuracy            : 0.805
# AUC                          : 0.815
# Balanced Error Rate          : 0.195

# sensitivity and specificity will always be inversely related (i.e., one increases as the other decreases). With the balanced data using under sampling on the model (e) the specificity has been increased whereas sensitivity has been decreased, but the overall balanced accuracy has been increased, ppv value increased.
```

